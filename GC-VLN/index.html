<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation">
    <meta name="author" content="Hang Yin,
                                 Haoyu Wei,
                                 Xiuwei Xu,
                                 Wenxuan Guo,
                                 Jie Zhou,
                                 Jiwen Lu">

    <title>GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation</h2>
    <h3>CoRL 2025</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p>
        <span style="white-space: nowrap; font-size:larger">
        <a href="https://bagh2178.github.io/">Hang Yin</a><sup>1*</sup>&nbsp;&nbsp;
        <a href="https://Why-peace.github.io/">Haoyu Wei</a><sup>1*</sup>&nbsp;&nbsp;
        <a href="https://xuxw98.github.io/">Xiuwei Xu</a><sup>1†</sup>&nbsp;&nbsp;
        <a href="https://gwxuan.github.io/">Wenxuan Guo</a><sup>1</sup>&nbsp;&nbsp;
        <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a><sup>1</sup>&nbsp;&nbsp;
        <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a><sup>1‡</sup>
        </span>
        <br><br>
        <sup>1</sup>Tsinghua University<br>
        <br><br>
        <a href="https://arxiv.org/abs/2509.10454" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/file.png" alt="paper" style="vertical-align: middle;">
            &nbsp;Paper (arXiv)
        </a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://github.com/bagh2178/GC-VLN" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/github.png" alt="code" style="vertical-align: middle;">
            &nbsp;Code (GitHub)
        </a>&nbsp;&nbsp;&nbsp;&nbsp;
    </p>

    <!-- <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2006.09661">Paper</a>
        <a class="btn btn-primary" href="https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb">Colab Notebook</a>
        <a class="btn btn-primary" href="https://dcato98.github.io/playground/#activation=sine">Tensorflow Playground</a>
        <a class="btn btn-primary" href="https://github.com/vsitzmann/siren">Code</a>
        <a class="btn btn-primary" href="https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K">Data</a>
    </div> -->
</div>

<div class="container">

        <div class="vcontainer">
        <iframe class='video' src="https://www.youtube.com/embed/D-DBaBp_trQ" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
        </div>
        <p>
            If video does not load, click <a href="https://cloud.tsinghua.edu.cn/f/f21a13df2bc749bb980a/?dl=1">HERE</a> to download.
        </p>

    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
            In this paper, we propose a training-free framework for vision-and-language navigation (VLN). Existing zero-shot VLN methods are mainly designed for discrete environments or involve unsupervised training in continuous simulator environments, which makes it challenging to generalize and deploy them in real-world scenarios. To achieve a training-free framework in continuous environments, our framework formulates navigation guidance as graph constraint optimization by decomposing instructions into explicit spatial constraints. The constraint-driven paradigm decodes spatial semantics through constraint solving, enabling zero-shot adaptation to unseen environments. Specifically, we construct a spatial constraint library covering all types of spatial relationship mentioned in VLN instructions. The human instruction is decomposed into a directed acyclic graph, with waypoint nodes, object nodes and edges, which are used as queries to retrieve the library to build the graph constraints. The graph constraint optimization is solved by the constraint solver to determine the positions of waypoints, obtaining the robot's navigation path and final goal. To handle cases of no solution or multiple solutions, we construct a navigation tree and the backtracking mechanism. Extensive experiments on standard benchmarks demonstrate significant improvements in success rate and navigation efficiency compared to state-of-the-art zero-shot VLN methods. We further conduct real-world experiments to show that our framework can effectively generalize to new environments and instruction sets, paving the way for a more robust and autonomous navigation framework.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/teaser.png" alt="teaser" width="95%">
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Approach</h2>
        <hr>
        <p>
            <b>Framework of GC-VLN.</b> We construct a constraint library, containing all the spatial relationship mentioned by navigation instruction. The instruction is decomposed into a directed acyclic graph (DAG) and used to query the library to get the graph constraints. The constraint solver determines the path by solving the graph constraint optimization. According to the topological sort (TS) of graph constraint, we build the navigation tree, where the number of leaf nodes equals the number of graph constraint solutions. In graph constraint and TS, t.i means the i-th object node in stage t.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/pipeline.png" alt="pipeline" width="90%">
            </div>
        </div>
    </div>


    <div class="section">
        <h2>Experiments</h2>
        <hr>
        <p>
            We evaluate our method on R2R-CE and RxR-CE.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/exp1.png" alt="exp1" width="85%">
            </div>
        </div>
        <p>
            <b>Navigation results</b> on R2R-CE and RxR-CE. We mainly compare the Success Rate (SR) and success rate weighted by path length (SPL) of state-of-the-art methods in different settings.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/exp2.png" alt="exp2" width="85%">
            </div>
        </div>
        <p>
            <b>Demonstration of the graph constraints solving of GC-VLN.</b> Here t_i is the i-th branch node of the t-th level of navigation tree.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/exp3.png" alt="exp2" width="85%">
            </div>
        </div>
        <p>
            <b>Demonstration of deployment in real-world environment.</b>
        </p>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
            <div class="bibtexsection">
                @article{yin2025gcvln, 
                      title={GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation}, 
                      author={Hang Yin and Haoyu Wei and Xiuwei Xu and Wenxuan Guo and Jie Zhou and Jiwen Lu},
                      journal={arXiv preprint arXiv:2509.10454},
                      year={2025}
                }
            </div>
        </div>
    </div>

    <hr>
</div>

    <p><center>
        <div id="clustrmaps-widget" style="width:30%">
            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=uMXvhM4rasxGQ4NtTbjPyYSnR25jCAOJ7YSIw1bpgwY&cl=ffffff&w=a"></script>
        </div>        
        <br>
        &copy; Hang Yin | Last update: Sep. 5, 2025
    </center></p>

<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
