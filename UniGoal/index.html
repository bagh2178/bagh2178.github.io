<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="UniGoal: Towards Universal Zero-shot Goal-oriented Navigation">
    <meta name="author" content="Hang Yin,
                                 Xiuwei Xu,
                                 Lingqing Zhao,
                                 Ziwei Wang,
                                 Jie Zhou,
                                 Jiwen Lu">

    <title>UniGoal: Towards Universal Zero-shot Goal-oriented Navigation</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>UniGoal: Towards Universal Zero-shot Goal-oriented Navigation</h2>
    <h3>CVPR 2025</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p>
        <span style="white-space: nowrap; font-size:larger">
        <a href="https://bagh2178.github.io/">Hang Yin</a><sup>1</sup>&nbsp;&nbsp;
        <a href="https://xuxw98.github.io/">Xiuwei Xu</a><sup>1†</sup>&nbsp;&nbsp;
        <a href="https://lqzhao.github.io/">Lingqing Zhao</a><sup>1</sup>&nbsp;&nbsp;
        <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a><sup>2</sup>&nbsp;&nbsp;
        <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a><sup>1</sup>&nbsp;&nbsp;
        <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a><sup>1‡</sup>
        </span>
        <br><br>
        <sup>1</sup>Tsinghua University&nbsp;&nbsp;<sup>2</sup>Nanyang Technological University<br>
        <br><br>
        <a href="https://arxiv.org/abs/2410.08189" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/file.png" alt="paper" style="vertical-align: middle;">
            &nbsp;Paper (arXiv)
        </a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://github.com/bagh2178/UniGoal" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/github.png" alt="code" style="vertical-align: middle;">
            &nbsp;Code (GitHub)
        </a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://zhuanlan.zhihu.com/p/909651478" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/zhihu.png" alt="code" style="vertical-align: middle;">
            &nbsp;中文解读 (Zhihu)
        </a>
    </p>

    <!-- <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2006.09661">Paper</a>
        <a class="btn btn-primary" href="https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb">Colab Notebook</a>
        <a class="btn btn-primary" href="https://dcato98.github.io/playground/#activation=sine">Tensorflow Playground</a>
        <a class="btn btn-primary" href="https://github.com/vsitzmann/siren">Code</a>
        <a class="btn btn-primary" href="https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K">Data</a>
    </div> -->
</div>

<div class="container">

        <div class="vcontainer">
        <iframe class='video' src="https://www.youtube.com/embed/Br-yuB1Bxco" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
        </div>
        <p>
            If video does not load, click <a href="https://cloud.tsinghua.edu.cn/f/ae050a060d624be4bc5d/?dl=1">HERE</a> to download.
        </p>

    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
            In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning. Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages. Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/teaser.png" alt="pipeline" width="100%">
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Approach</h2>
        <hr>
        <p>
            <b>Overall framework of our approach.</b> We construct a hierarchical 3D scene graph as well as an occupancy map online. At each step, we divide the scene graph into several subgraphs, each of which is prompted to LLM with a hierarchical chain-of-thought for structural understanding of the scene context. We interpolate the probability score of each subgraph to the frontiers and select the frontier with highest score for exploration. This decision is also explainable by summarizing the reasoning process of the LLM. With the scene graph representation, we further design a re-perception mechanism, which helps the agent give up false positive goal object by continuous credibility judgement.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/pipeline.png" alt="pipeline" width="90%">
            </div>
        </div>
    </div>


    <div class="section">
        <h2>Experiments</h2>
        <hr>
        <p>
            We evaluate our method on MP3D, HM3D and RoboTHOR.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/exp1.png" alt="pipeline" width="85%">
            </div>
        </div>
        <p>
            <b>Object-goal navigation results</b> on MP3D, HM3D and RoboTHOR. We compare the Success Rate (SR) and success rate weighted by
path length (SPL) of state-of-the-art methods in different settings.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/exp2.png" alt="pipeline" width="70%">
            </div>
        </div>
        <p>
            <b>Left:</b> Per category Success Rate on MP3D. <b>Right:</b> Time cost of connecting n edges for online 3D scene graph construction. 
        </p>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
            <div class="bibtexsection">
                @article{yin2024sgnav, 
                      title={SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation}, 
                      author={Hang Yin and Xiuwei Xu and Zhenyu Wu and Jie Zhou and Jiwen Lu},
                      journal={arXiv preprint arXiv:2410.08189},
                      year={2024}
                }
            </div>
        </div>
    </div>

    <hr>
</div>

    <p><center>
        <div id="clustrmaps-widget" style="width:30%">
            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=7gRzdEL_6XBpmGGsJ8nyhx_XX1R-LS79luWEJFp_bME&cl=ffffff&w=a"></script>    
        </div>        
        <br>
        &copy; Hang Yin | Last update: Mar. 9, 2025
    </center></p>

<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
